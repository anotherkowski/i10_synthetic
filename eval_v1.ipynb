{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Navigation\n","1. [Start Here](hey.ipynb)\n","1. [Load Data and Clean](/eda.ipynb)\n","1. [To Clean, or Not To Clean?](eval_v1.ipynb)\n","1. Generate Datasets\n","    1. [Faker Naive](faker_naive.ipynb)\n","    1. [Faker Plus](faker_plus.ipynb)\n","    1. [SDV Naive](sdv_v1.ipynb)\n","    1. [SDV More Better](sdv_v2.ipynb)\n","    1. [SDV TVAE]()\n","1. Compare and Evaluate Performance\n","    1. [First impressions](eval_v2.ipynb)\n","    1. [Loan financial models](eval_v3.ipynb)\n","    1. [Predicting default risk](eval_v4.ipynb)\n","    1. [How hackable]()"]},{"cell_type":"markdown","metadata":{},"source":["# Initial Performance Evaluation\n","#### Raw vs. Cleaned vs. Synthetic on Raw vs. Synthetic on Cleaned"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"33051560f24143ad977076e2b6653ae4","deepnote_cell_type":"code","execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":1611,"execution_start":1728651172618,"source_hash":"846c21d"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score\n","\n","# Display all the things\n","pd.set_option('display.float', '{:.2f}'.format)\n","pd.set_option('display.max_columns', 50)\n","pd.set_option('display.max_rows', 200)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"9382eb1ab0e8459aab7145612f493b24","deepnote_cell_type":"code","execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":28150,"execution_start":1728651174308,"source_hash":"8e487197"},"outputs":[],"source":["# Import the raw data\n","# Drop all columns *except* for loan_amnt, loan_status, and dti\n","raw_data = pd.read_csv(\"FILEPATH\", low_memory=False, compression='gzip')"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8b2f417433b1492cadfbe27c048afd51","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":1678,"execution_start":1728614904670,"source_hash":"ea389c34"},"outputs":[],"source":["# Count the unusable rows because they contain NaN values\n","print('Number of unusable rows in raw data:', raw_data[['loan_amnt', 'loan_status', 'dti']].isna().sum())\n","raw_data = raw_data.dropna(subset=['loan_amnt', 'loan_status', 'dti'])\n","raw_data = raw_data[['loan_amnt', 'loan_status', 'dti']]\n","# Drop rows where loan_amnt is under $1000\n","print('Number of too small loan amount rows in raw data:', raw_data['loan_amnt'].lt(1000).sum())\n","raw_data = raw_data[raw_data['loan_amnt'] >= 1000]\n","raw_data.info(verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"fcee3841d1534c49b7d44a5c0fb4d479","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":1,"execution_start":1728614906398,"source_hash":"f298c5ac"},"outputs":[],"source":["# Create the same number of rows for the synthetic datasets\n","n_rows = len(raw_data)\n","n_rows"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f186f829e6a140ccac0d2fd8a7e81b92","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":1873,"execution_start":1728614906446,"source_hash":"fb8adb8"},"outputs":[],"source":["# Synthesize the raw data\n","import sdv\n","from sdv.metadata import Metadata\n","\n","metadata = Metadata.detect_from_dataframe(data=raw_data)\n","metadata.save_to_json(filepath='raw_synth_metadata_v1.json')\n","\n","from sdv.single_table import GaussianCopulaSynthesizer\n","\n","# Step 1: Create the synthesizer\n","synthesizer = GaussianCopulaSynthesizer(metadata)"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"555bd52d3268401f95bb5592ce04b9d5","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":123748,"execution_start":1728614908366,"source_hash":"44c17edb"},"outputs":[],"source":["# Step 2: Train the synthesizer\n","synthesizer.fit(raw_data)"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"45bd339443bd47c9b73bc1cf3775a66b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":12329,"execution_start":1728615032162,"source_hash":"ba0785f6"},"outputs":[],"source":["# Step 3: Generate synthetic data\n","synthesized_from_raw = synthesizer.sample(num_rows=n_rows)"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"5728457248d94d06b2dbb7ad4ee0970c","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":1006,"execution_start":1728615044542,"source_hash":"c0c8e896"},"outputs":[],"source":["# Import my cleaned data\n","cleaned_data = pd.read_csv(\"FILEPATH\", low_memory=False, compression='gzip')\n","# Count the unusable rows because they contain NaN values\n","print('Number of unusable rows in cleaned data:', cleaned_data[['loan_amnt', 'loan_status', 'dti']].isna().sum())\n","cleaned_data = cleaned_data.dropna(subset=['loan_amnt', 'loan_status', 'dti'])\n","cleaned_data = cleaned_data[['loan_amnt', 'loan_status', 'dti']]\n","# Drop rows where loan_amnt is under $1000\n","print('Number of too small loan amount rows in cleaned data:', cleaned_data['loan_amnt'].lt(1000).sum())\n","cleaned_data = cleaned_data[cleaned_data['loan_amnt'] >= 1000]\n","cleaned_data.info(verbose=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"522757a111ba4f7e94e5e772f28e46ff","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":34072,"execution_start":1728234657780,"source_hash":"228d206a"},"outputs":[],"source":["# Default synthesized from clean\n","# Synthesize the raw data\n","import sdv\n","from sdv.metadata import Metadata\n","\n","metadata = Metadata.detect_from_dataframe(data=raw_data)\n","metadata.save_to_json(filepath='clean_synth_metadata_v1.json')\n","\n","from sdv.single_table import GaussianCopulaSynthesizer\n","\n","# Step 1: Create the synthesizer\n","synthesizer = GaussianCopulaSynthesizer(metadata)\n","\n","# Step 2: Train the synthesizer\n","synthesizer.fit(cleaned_data)\n","\n","# Step 3: Generate synthetic data\n","synthesized_from_clean = synthesizer.sample(num_rows=len(cleaned_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"bf2c7fed00cc4af8974cafe6e96c5293","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":348568,"execution_start":1728235747724,"source_hash":"5ff28a"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.exceptions import ConvergenceWarning\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n","\n","def evaluate_models(raw_data, clean_data, synthesized_from_raw, synthesized_from_clean):\n","    # Define a function to train and evaluate a model\n","    def train_and_evaluate(X, y):\n","        # Split into training and test sets\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","        \n","        # Train the model\n","        model = LogisticRegression(max_iter=500)  # Increased max_iter to 500\n","        model.fit(X_train, y_train)\n","        \n","        # Make predictions\n","        predictions = model.predict(X_test)\n","        \n","        # Calculate metrics\n","        accuracy = accuracy_score(y_test, predictions)\n","        \n","        # Determine the averaging method for F1 score based on the number of unique classes\n","        unique_classes = np.unique(y)\n","        average_method = 'binary' if len(unique_classes) == 2 else 'macro'  # Use 'macro' for multiclass\n","        \n","        f1 = f1_score(y_test, predictions, average=average_method, labels=unique_classes)\n","        return accuracy, f1\n","\n","    # Prepare the raw real data\n","    X_raw = raw_data.drop('loan_status', axis=1)\n","    y_raw = raw_data['loan_status']\n","    accuracy_raw, f1_raw = train_and_evaluate(X_raw, y_raw)\n","\n","    # Prepare the clean real data\n","    X_clean = clean_data.drop('loan_status', axis=1)\n","    y_clean = clean_data['loan_status']\n","    accuracy_clean, f1_clean = train_and_evaluate(X_clean, y_clean)\n","\n","    # Prepare the synthesized data trained on raw data\n","    X_synthesized_raw = synthesized_from_raw.drop('loan_status', axis=1)\n","    y_synthesized_raw = synthesized_from_raw['loan_status']\n","    accuracy_synthesized_raw, f1_synthesized_raw = train_and_evaluate(X_synthesized_raw, y_synthesized_raw)\n","\n","    # Prepare the synthesized data trained on clean data\n","    X_synthesized_clean = synthesized_from_clean.drop('loan_status', axis=1)\n","    y_synthesized_clean = synthesized_from_clean['loan_status']\n","    accuracy_synthesized_clean, f1_synthesized_clean = train_and_evaluate(X_synthesized_clean, y_synthesized_clean)\n","\n","    # Create a DataFrame to store the results\n","    results_df = pd.DataFrame({\n","        'Dataset': [\n","            'Raw Real Data',\n","            'Synthesized from Raw Data',\n","            'Clean Real Data',\n","            'Synthesized from Clean Data'\n","        ],\n","        'Accuracy': [\n","            accuracy_raw,\n","            accuracy_synthesized_raw,\n","            accuracy_clean,\n","            accuracy_synthesized_clean\n","        ],\n","        'F1 Score': [\n","            f1_raw,\n","            f1_synthesized_raw,\n","            f1_clean,\n","            f1_synthesized_clean\n","        ]\n","    })\n","\n","    # Print the results DataFrame\n","    print(results_df)\n","    results_df.to_csv('results.csv', index=False)\n","\n","# Evaluate the models\n","evaluate_models(raw_data, cleaned_data, synthesized_from_raw, synthesized_from_clean)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ebf8d93ed7454af3b06a9b60d79b995b","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_context_id":"7e36cc7a-0e26-4347-80ba-4e36fa088e00","execution_millis":6515,"execution_start":1728236096344,"source_hash":"d9b2f508"},"outputs":[],"source":["\"\"\"\n","VISUALIZE THE RESULTS\n","\"\"\"\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","results_df = pd.read_csv('results.csv')\n","\n","# Set up the visualizations\n","plt.figure(figsize=(12, 5))\n","\n","# Plot Accuracy\n","plt.subplot(1, 2, 1)\n","sns.barplot(x='Dataset', y='Accuracy', data=results_df, palette='viridis')\n","plt.title('Model Accuracy by Dataset')\n","plt.ylim(0, 1)\n","plt.ylabel('Accuracy')\n","plt.xticks(rotation=15)\n","\n","# Plot F1 Score\n","plt.subplot(1, 2, 2)\n","sns.barplot(x='Dataset', y='F1 Score', data=results_df, palette='viridis')\n","plt.title('F1 Score by Dataset')\n","plt.ylim(0, 1)\n","plt.ylabel('F1 Score')\n","plt.xticks(rotation=15)\n","\n","# Show the plots\n","plt.tight_layout()\n","plt.show()\n","plt.savefig('clean_vs_raw_results.png')"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"7de4dc1020f74f71b51e186480316f63","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
