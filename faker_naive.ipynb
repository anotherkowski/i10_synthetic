{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Navigation\n","1. [Start Here](hey.ipynb)\n","1. [Load Data and Clean](/eda.ipynb)\n","1. [To Clean, or Not To Clean?](eval_v1.ipynb)\n","1. Generate Datasets\n","    1. [Faker Naive](faker_naive.ipynb)\n","    1. [Faker Plus](faker_plus.ipynb)\n","    1. [SDV Naive](sdv_v1.ipynb)\n","    1. [SDV More Better](sdv_v2.ipynb)\n","    1. [SDV TVAE]()\n","1. Compare and Evaluate Performance\n","    1. [First impressions](eval_v2.ipynb)\n","    1. [Loan financial models](eval_v3.ipynb)\n","    1. [Predicting default risk](eval_v4.ipynb)\n","    1. [How hackable]()"]},{"cell_type":"markdown","metadata":{"cell_id":"c9841d44bd3c4d40a856e994b6b2e73b","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Naive Synthetic Data Generation using Faker()"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"74ce5cfaee144777a3de1b4ec9972b39","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":779,"execution_start":1728262990925,"source_hash":"e6613cbd"},"outputs":[],"source":["# Import libraries I use in this notebook\n","import os\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import random\n","from random import randint\n","\n","# \"magic\" command to make plots show up in the notebook\n","%matplotlib inline\n","sns.set_style('whitegrid')\n","\n","# Display all the things\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.max_rows', 200)\n","\n","# Initiatlize the Faker library\n","from faker import Faker\n","fake = Faker()"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"16608dce6b6a4375b878def1137f8d58","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":9099,"execution_start":1728262991749,"source_hash":"f66a75a7"},"outputs":[],"source":["# Import the rejected dataset (post cleansing) into a Pandas DataFrame\n","accepted_df = pd.read_csv('FILEPATH', compression='gzip', low_memory=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4d3f1bbd012b4f6e894d4070b8e965f9","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":335,"execution_start":1728263000897,"source_hash":"a94cb709"},"outputs":[],"source":["# Do all columns in rejected_df have the same count of items?\n","accepted_df.count()"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"ecfd35bb4b0c42879b4200ceb70b55b1","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":0,"execution_start":1728263001277,"source_hash":"90211f34"},"outputs":[],"source":["# Store our total item count so we can make the right number of rows later\n","n_rows = len(accepted_df)"]},{"cell_type":"markdown","metadata":{"cell_id":"e859dc2c42924219b16018e92e0ee931","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Instead of generating completely random data, let's try to be slightly less naive and attempt to preserve some of the statistical properties of the original. That means, we first need to establish the min and max of float columns, range for integers, and labels for the categorial variables."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2885d27bb9124d79b1f250baa8d515d2","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":247,"execution_start":1728263001325,"source_hash":"5a95ffe2"},"outputs":[],"source":["# Find the min and max for all numeric columns\n","# Save to a csv and write that metadata to a dictionary I can use later with Faker\n","\n","def process_df(df):\n","    \"\"\"\n","    Process the DataFrame to compute minimum and maximum values for numeric columns.\n","\n","    Args:\n","        df (pd.DataFrame): The input DataFrame to process.\n","\n","    Returns:\n","        pd.DataFrame: A new DataFrame containing column names, min, and max values.\n","    \"\"\"\n","    # Initialize an empty list to store results\n","    res = []\n","    \n","    # Keep track of successfully processed column names\n","    proc_cols = []\n","\n","    # Iterate through each column in the original DataFrame\n","    for col in df.columns:\n","        if pd.api.types.is_numeric_dtype(df[col]):\n","            try:\n","                # Calculate minimum and maximum for numeric columns\n","                min_val = df[col].min()\n","                max_val = df[col].max()\n","\n","                # Append the results to the list\n","                res.append({\n","                    'Col': col,   # Shortened column name\n","                    'Min': min_val,  # Shortened column name\n","                    'Max': max_val   # Shortened column name\n","                })\n","\n","                # Add the column name to the processed list\n","                proc_cols.append(col)\n","\n","            except Exception as e:\n","                print(f\"Error processing column '{col}': {e}\")\n","        else:\n","            # Skip non-numeric columns and print a message\n","            print(f\"Skipping non-numeric column: '{col}'\")\n","\n","    # Check if any columns were processed successfully\n","    if not res:\n","        print(\"No numeric columns found to process.\")\n","        return pd.DataFrame(columns=['Col', 'Min', 'Max'])  # Return an empty DataFrame with proper columns\n","\n","    # Create a new DataFrame from the results\n","    new_df = pd.DataFrame(res)\n","\n","    # Print total columns processed successfully and their names\n","    print(f\"Total columns processed successfully: {len(proc_cols)}\")\n","    print(\"Processed columns:\", proc_cols)\n","\n","    return new_df\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    # Process the DataFrame\n","    result_df = process_df(accepted_df)\n","\n","    # Save the new DataFrame to a CSV file\n","    result_df.to_csv('processed_data.csv', index=False)\n","\n","    # Convert the DataFrame to a dictionary for later use\n","    result_dict = result_df.set_index('Col').T.to_dict()\n","    \n","    # Print the resulting dictionary\n","    print(\"Resulting dictionary:\", result_dict)\n","\n","    # Display the new DataFrame\n","    print(result_df)"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"96b7c94fa2e6456da718419accb09ec7","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":0,"execution_start":1728263001617,"source_hash":"ec3024e7"},"outputs":[],"source":["# Create a new dataframe\n","faker_dfa = pd.DataFrame()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"da45ab5ce9d94a88a6dd9649f2ca3cc4","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":3427,"execution_start":1728263001661,"source_hash":"38bec154"},"outputs":[],"source":["# Generate rows of data for the dates\n","def random_dates(start, end, n, freq, seed=None):\n","    if seed is not None:\n","        np.random.seed(seed)\n","\n","    dr = pd.date_range(start, end, freq=freq)\n","    return pd.to_datetime(np.sort(np.random.choice(dr, n)))\n","\n","faker_dfa['issue_d'] = random_dates('2013-01-01', '2017-12-31', n_rows, 'B')\n","faker_dfa['issue_d'] = faker_dfa['issue_d'].dt.strftime('%Y')\n","faker_dfa['issue_d'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1166e135ff534b0aa7b3cd5862c5edb6","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":0,"execution_start":1728263005129,"source_hash":"447954c4"},"outputs":[],"source":["# Find the range of earliest_cr_line\n","accepted_df['earliest_cr_line'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b40d010434c149b9ab3411ceab92d735","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":2420,"execution_start":1728263005177,"source_hash":"d6f041a5"},"outputs":[],"source":["# Find the range of issue_d\n","import datetime\n","accepted_df['earliest_cr_line'] = pd.to_datetime(accepted_df['earliest_cr_line'])\n","mini = accepted_df['earliest_cr_line'].dt.year.min()\n","maxi = accepted_df['earliest_cr_line'].dt.year.max()\n","faker_dfa['earliest_cr_line'] = [randint(mini, maxi) for x in range(n_rows)]\n","faker_dfa['earliest_cr_line'].describe().apply(lambda x: format(x, 'f'))"]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"80123ad59ae64f96ac72e1479587abb3","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":13556,"execution_start":1728263007641,"source_hash":"2cf1a28f"},"outputs":[],"source":["# Generate rows of data for each column in the result dictionary\n","# Add new columns the same way by looping through the columns in the results dictionary\n","for col in result_dict:\n","    mini = int(result_dict[col]['Min'])\n","    maxi = int(result_dict[col]['Max'])\n","    faker_dfa[col] = [randint(mini, maxi) for x in range(n_rows)]\n","    faker_dfa[col].describe().apply(lambda x: format(x, 'f'))"]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"b9cd1e4f4c8344268d90a6df7434cdb1","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":3069,"execution_start":1728263021261,"source_hash":"d9500940"},"outputs":[],"source":["# Use faker to generate zip for faker_dfa and states for addr_state\n","faker_dfa['addr_state'] = [fake.state_abbr() for _ in range(n_rows)]"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4842931b50694f9db5ab48f1d884587c","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":0,"execution_start":1728263024377,"source_hash":"169e413c"},"outputs":[],"source":["# How many columns are in accepted_df but are not in our faker_dfa? Print the names of the columns\n","missing_cols = accepted_df.columns.difference(faker_dfa.columns)\n","print(missing_cols)"]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"5e2f22467b2b414586e95dedb53f0470","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":416,"execution_start":1728263024433,"source_hash":"46bd16ec"},"outputs":[],"source":["# Process the remaining columns as categorical columns from missing_cols\n","# Loop through missing_cols and populate with random elements\n","for col in missing_cols:\n","    # Get unique elements once\n","    unique_elements = accepted_df[col].unique()\n","    # Use NumPy to generate random choices efficiently\n","    faker_dfa[col] = np.random.choice(unique_elements, size=n_rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"11c82b452f534606b0eff8f866c7d64d","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":284,"execution_start":1728263024925,"source_hash":"4b59c63a"},"outputs":[],"source":["# Take a quick look at our new dataframe\n","faker_dfa.info(verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7fc2f379ee414ec7800a8dcb0c662147","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":0,"execution_start":1728263025253,"source_hash":"1f2a3057"},"outputs":[],"source":["# Find any difference between the column names and the faker created dataframe\n","print(faker_dfa.columns.difference(accepted_df.columns))\n","print(accepted_df.columns.difference(faker_dfa.columns))"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"44768be07174473b9d55227faa24c375","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":4572,"execution_start":1728263026423,"source_hash":"b7f989c9"},"outputs":[],"source":["# Remove the old dataframe\n","del accepted_df\n","# Take a look at our memory usage\n","faker_dfa.memory_usage(index=False, deep=True).to_csv('FILEPATH')\n","faker_dfa.memory_usage(index=False, deep=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"cell_id":"6c54bb44c7f54999b5975e5b7506fec2","deepnote_cell_type":"code","execution_context_id":"cb15bcbe-911b-4234-a696-b028a5cc6b43","execution_millis":14141,"execution_start":1728263084749,"source_hash":"7c2afb2b"},"outputs":[],"source":["# Save our new faker created dataframe\n","faker_dfa.to_csv('FILEPATH', index=False)"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"162e93772f914937ad211e8651019546","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
