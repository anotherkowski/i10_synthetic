{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Navigation\n","1. [Start Here](hey.ipynb)\n","1. [Load Data and Clean](/eda.ipynb)\n","1. [To Clean, or Not To Clean?](eval_v1.ipynb)\n","1. Generate Datasets\n","    1. [Faker Naive](faker_naive.ipynb)\n","    1. [Faker Plus](faker_plus.ipynb)\n","    1. [SDV Naive](sdv_v1.ipynb)\n","    1. [SDV More Better](sdv_v2.ipynb)\n","    1. [SDV TVAE]()\n","1. Compare and Evaluate Performance\n","    1. [First impressions](eval_v2.ipynb)\n","    1. [Loan financial models](eval_v3.ipynb)\n","    1. [Predicting default risk](eval_v4.ipynb)\n","    1. [How hackable]()"]},{"cell_type":"markdown","metadata":{},"source":["# Faker Plus\n","#### Faker, but more better"]},{"cell_type":"markdown","metadata":{"cell_id":"a9aaa7c53a9444f68bb06221b633b83c","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["The most naive version of using Faker to generate data leaves much to be desired. Here is a custom module that will generate slightly more sophisticated Faker data that better matches the statistical properties of the original data set."]},{"cell_type":"markdown","metadata":{"cell_id":"08d165d0b390453e9c6702f925f93a57","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Note that this should not be used for personally identifiable information such as names and addresses because this module will simply replicate the original string values, treating them like they are categorical."]},{"cell_type":"markdown","metadata":{"cell_id":"853305bc188942fa8e04fe0d4da2fbcd","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["These results will be compared to the most simplistic Faker() generated data sets. I have affectionately named this \"Faker Plus\"."]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"1428bbf04bcf4b299e2de7f6a970f06c","deepnote_cell_type":"code","execution_context_id":"90ab8de0-bf6a-4504-be55-ab81036134a8","execution_millis":111,"execution_start":1728263200485,"source_hash":"17137579"},"outputs":[],"source":["# Make a more sophisticated Faker dataset that maintains more of the original statistical properties of the original dataset.\n","# Including min, max, mean, stddev, and frequency distributions for categorical variables\n","# Metadata for these are stored in a JSON file\n","\n","## \n","\"\"\"\n","The below is a test of the DataFrameGenerator class using an example JSON file.\n","\"\"\"\n","##\n","import pandas as pd\n","import numpy as np\n","from faker import Faker\n","import json\n","import logging\n","fake = Faker()\n","import random\n","import os, sys\n","\n","# Configure logging\n","logging.basicConfig(\n","    filename='fauxnalysis.log',\n","    level=logging.DEBUG,\n","    format='%(asctime)s - %(levelname)s - %(message)s'\n",")\n","\n","class DataFrameGenerator:\n","    def __init__(self, config):\n","        \"\"\"Initialize the DataFrameGenerator with a configuration.\"\"\"\n","        self.config = config\n","        self.fake = Faker()\n","        self.n_rows = config.get('n_rows', 10)\n","        self.data = {}\n","\n","    def generate_dataframe(self):\n","        \"\"\"Generate a DataFrame based on the configuration.\"\"\"\n","        for col in self.config['columns']:\n","            col_name = col['name']\n","            col_type = col['type']\n","            null_count = col.get('null_count', 0)\n","\n","            self.data[col_name] = self.generate_column_data(col_type, col)\n","\n","            # Introduce null values\n","            if null_count > 0:\n","                self._introduce_nulls(col_name, null_count)\n","\n","        logging.info(\"Generated fake DataFrame.\")\n","        return pd.DataFrame(self.data)\n","\n","    def generate_column_data(self, col_type, col_config):\n","        \"\"\"Generate data for a specific column type.\"\"\"\n","        generator = {\n","            'categorical': self._generate_categorical_data,\n","            'float': self._generate_numerical_data,\n","            'int': self._generate_numerical_data,\n","            'bool': lambda _: np.random.choice([True, False], self.n_rows),\n","            'state': self._generate_state_data,\n","            'bothify': self._generate_bothify_data,\n","            'string': lambda _: [self.fake.sentence() for _ in range(self.n_rows)],\n","            'yar': lambda _: [self.fake.year() for _ in range(self.n_rows)],\n","            'date': lambda _: [self._generate_date() for _ in range(self.n_rows)]\n","        }\n","\n","        if col_type in generator:\n","            return generator[col_type](col_config)\n","        \n","        raise ValueError(f\"Unsupported data type: {col_type}\")\n","    \n","    def _generate_date(self):\n","        year = fake.year()\n","        if int(year) < 2019 and int(year) > 2012:\n","            random_date = f'{year}'\n","            return random_date\n","        else:\n","            return self._generate_date()\n","\n","\n","    def _introduce_nulls(self, col_name, null_count):\n","        \"\"\"Randomly introduce null values into the specified column.\"\"\"\n","        indices = np.random.choice(self.n_rows, null_count, replace=False)\n","        for index in indices:\n","            self.data[col_name][index] = None\n","\n","    def _generate_numerical_data(self, col_config):\n","        \"\"\"Generate numerical data based on statistical parameters.\"\"\"\n","        min_val = col_config.get('min')\n","        max_val = col_config.get('max')\n","\n","        if min_val is None or max_val is None:\n","            raise ValueError(\"Both 'min' and 'max' values must be specified.\")\n","\n","        mean = col_config.get('mean', (min_val + max_val) / 2)\n","        stddev = col_config.get('stddev', (max_val - min_val) / 6)\n","\n","        col_type = col_config['type']\n","        if col_type in ['float', 'curr']:\n","            data = np.random.normal(loc=mean, scale=stddev, size=self.n_rows)\n","        elif col_type == 'int':\n","            data = np.random.randint(min_val, max_val, self.n_rows)\n","        else:\n","            raise ValueError(\"Unsupported numerical type.\")\n","\n","        # Clip the data to be within min and max values, ensuring we handle None\n","        data = np.clip(data, min_val, max_val)\n","        return data.tolist()  # Convert to list to avoid potential None issues\n","\n","    def _generate_bothify_data(self, col_config):\n","        \"\"\"Generate data using Faker's bothify method with a custom format.\"\"\"\n","        format_string = col_config.get('format', '???###')\n","        return [self.fake.bothify(format_string) for _ in range(self.n_rows)]\n","\n","    def _generate_state_data(self, col_config):\n","        \"\"\"Generate random state names based on frequency distribution.\"\"\"\n","        use_abbr = col_config.get('abbreviation', False)\n","        states = [self.fake.state_abbr() if use_abbr else self.fake.state() for _ in range(self.n_rows)]\n","\n","        # If a distribution is provided, sample according to it\n","        if 'distribution' in col_config:\n","            distribution = col_config['distribution']\n","            state_names = list(distribution.keys())\n","            probabilities = list(distribution.values())\n","            states = np.random.choice(state_names, self.n_rows, p=probabilities)\n","\n","        return states\n","\n","    def _generate_categorical_data(self, col_config):\n","        \"\"\"Generate categorical data based on the input series frequency.\"\"\"\n","        if 'distribution' not in col_config:\n","            raise ValueError(\"Frequency distribution must be provided for categorical data.\")\n","\n","        distribution = col_config['distribution']\n","        categories = list(distribution.keys())\n","        probabilities = list(distribution.values())\n","        # enforce probabilities sum to 1\n","        probabilities = [p / sum(probabilities) for p in probabilities]\n","        \n","        return np.random.choice(categories, self.n_rows, p=probabilities)\n","\n","class ConfigLoader:\n","    @staticmethod\n","    def load_config(json_file):\n","        \"\"\"Load column specifications from a JSON file.\"\"\"\n","        try:\n","            with open(json_file, 'r') as f:\n","                return json.load(f)\n","        except (FileNotFoundError, json.JSONDecodeError) as e:\n","            logging.error(f\"Error loading JSON file: {e}\")\n","            raise\n","\n","class Fauxnalysis:\n","    def __init__(self, config_file):\n","        \"\"\"Initialize the Fauxnalysis instance with a configuration file.\"\"\"\n","        self.config = ConfigLoader.load_config(config_file)\n","\n","    def generate_fake_data(self):\n","        \"\"\"Generate fake data and return it as a DataFrame.\"\"\"\n","        generator = DataFrameGenerator(self.config)\n","        return generator.generate_dataframe()\n","\n","    def save_to_csv(self, dataframe, filename):\n","        \"\"\"Save the DataFrame to a CSV file.\"\"\"\n","        dataframe.to_csv(filename, index=False, compression='gzip')\n","        logging.info(f\"DataFrame saved to {filename}\")"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"09f867860eef45689897da31446a1a5c","deepnote_cell_type":"code","execution_context_id":"90ab8de0-bf6a-4504-be55-ab81036134a8","execution_millis":9494,"execution_start":1728263200641,"source_hash":"fb9108f8"},"outputs":[],"source":["# Let's create a new config json file for the accepted data set\n","# Accepted dataframe\n","df = pd.read_csv('FILEPATH', compression='gzip', low_memory=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"37851b0ea71f49eb9bbb6c9442e04ad5","deepnote_cell_type":"code","execution_context_id":"90ab8de0-bf6a-4504-be55-ab81036134a8","execution_millis":1119,"execution_start":1728263210181,"source_hash":"453be73f"},"outputs":[],"source":["# Now process that dataframe to create json metadata file\n","# Function to process DataFrame and create JSON metadata\n","# Function to process DataFrame and create JSON metadata\n","def create_metadata_json(dataframe):\n","    metadata = []\n","\n","    for col in dataframe.columns:\n","        try:\n","            if pd.api.types.is_object_dtype(dataframe[col]):\n","                unique_count = dataframe[col].nunique()\n","                \n","                # Check if unique values exceed 25 for object columns only\n","                if unique_count > 50:\n","                    column_metadata = {\n","                        \"name\": col,\n","                        \"type\": \"string\"\n","                    }\n","                else:\n","                    # Calculate distribution for categorical data\n","                    value_counts = dataframe[col].value_counts(normalize=True).to_dict()\n","                    # Normalize the distribution to ensure it sums to 1\n","                    total = sum(value_counts.values())\n","                    distribution = {key: round(value / total, 2) for key, value in value_counts.items()}\n","                    \n","                    # Create the metadata entry for categorical columns\n","                    column_metadata = {\n","                        \"name\": col,\n","                        \"type\": \"categorical\",\n","                        \"distribution\": distribution\n","                    }\n","                metadata.append(column_metadata)\n","                \n","            elif pd.api.types.is_numeric_dtype(dataframe[col]):\n","                # Determine if the numeric column is float or integer\n","                if pd.api.types.is_float_dtype(dataframe[col]):\n","                    column_metadata = {\n","                        \"name\": col,\n","                        \"type\": \"float\",\n","                        \"min\": float(dataframe[col].min()),  # Convert to float\n","                        \"max\": float(dataframe[col].max()),  # Convert to float\n","                        \"mean\": float(dataframe[col].mean()),  # Convert to float\n","                        \"std_dev\": float(dataframe[col].std())  # Convert to float\n","                    }\n","                elif pd.api.types.is_integer_dtype(dataframe[col]):\n","                    column_metadata = {\n","                        \"name\": col,\n","                        \"type\": \"int\",\n","                        \"min\": int(dataframe[col].min()),  # Convert to int\n","                        \"max\": int(dataframe[col].max()),  # Convert to int\n","                        \"mean\": float(dataframe[col].mean()),  # Convert to float\n","                        \"std_dev\": float(dataframe[col].std())  # Convert to float\n","                    }\n","                else:\n","                    continue  # Skip non-standard numeric types\n","\n","                metadata.append(column_metadata)\n","                \n","        except Exception as e:\n","            print(f\"Error processing column '{col}': {e}\")\n","            continue  # Move on to the next column\n","\n","    return metadata\n","\n","# Generate the metadata\n","metadata = create_metadata_json(df)\n","\n","# Convert metadata to JSON string\n","metadata_json = json.dumps(metadata, indent=4)\n","\n","# Print the metadata JSON\n","print(metadata_json)\n","\n","# Save the JSON to a file\n","with open('metadata.json', 'w') as json_file:\n","    json_file.write(metadata_json)\n","\n","print(\"Metadata saved to 'metadata.json'.\")"]},{"cell_type":"markdown","metadata":{"cell_id":"7f2ec53229ae4514b19277e1da164b3e","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["To clean it up, I manually made edits to the metadata.json to ensure the \"faker plus\" generator was as accurate as possible."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"46f6a4a95acd4f0a81c2ca4696aa5677","deepnote_cell_type":"code","execution_context_id":"90ab8de0-bf6a-4504-be55-ab81036134a8","execution_millis":0,"execution_start":1728263211349,"source_hash":"ed3a83ad"},"outputs":[],"source":["# How many rows?\n","n_rows = len(df)\n","n_rows"]},{"cell_type":"markdown","metadata":{"cell_id":"cfd556137cfa49319b750308ce232866","color":"red","deepnote_cell_type":"text-cell-callout","formattedRanges":[]},"source":["> Stop here and make any needed edits to the metadata.json before continuing"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"271fc8fe85be4907ae57fd7bffc623fa","deepnote_cell_type":"code","execution_context_id":"90ab8de0-bf6a-4504-be55-ab81036134a8","execution_millis":92180,"execution_start":1728263398125,"source_hash":"36748a47"},"outputs":[],"source":["# Running the Fauxnalysis generator on my metadata.json\n","# Create an instance of Fauxnalysis\n","config_file = 'metadata.json'  # Ensure this file exists in the directory\n","fa = Fauxnalysis(config_file)\n","\n","# Generate the fake DataFrame\n","fake_df = fa.generate_fake_data()\n","\n","# Print the generated DataFrame\n","print(\"Generated Fake DataFrame:\")\n","fake_df.info(verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"b0969251a41e40b190b48f8d363c173d","deepnote_cell_type":"code","execution_context_id":"90ab8de0-bf6a-4504-be55-ab81036134a8","execution_millis":5521,"execution_start":1728263490607,"source_hash":"6b8ac2fb"},"outputs":[],"source":["# Take a look at our memory usage\n","fake_df.memory_usage(index=False, deep=True).to_csv('FILEPATH')\n","fake_df.memory_usage(index=False, deep=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"fc0f1871f46a425288635ad84d8b4f1e","deepnote_cell_type":"code","execution_context_id":"90ab8de0-bf6a-4504-be55-ab81036134a8","execution_millis":33400,"execution_start":1728263496169,"source_hash":"752a98c"},"outputs":[],"source":["# Save the DataFrame to a CSV file\n","fake_df.to_csv('FILEPATH', index=False)"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"65eee38fa2cc477fa1d9b2ec07701e29","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
